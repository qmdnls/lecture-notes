% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=0.7in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}

% times new roman
%\usepackage{newtxtext,newtxmath}

% baskerville
\usepackage{Baskervaldx}
\usepackage[baskervaldx]{newtxmath} 
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

% utf-8
\usepackage[utf8]{inputenc}

% cite color
\usepackage[x11names]{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true,%
citecolor=DodgerBlue4,%
filecolor=blue,%
linkcolor=blue,%
urlcolor=blue
}

% line spacing
\renewcommand{\baselinestretch}{1.0}

% margin
\usepackage{geometry}
 \geometry{
 a4paper,
 left=20mm,
 right=20mm,
 top=20mm,
 bottom=20mm
}

% name and student ID in header
\usepackage{fancyhdr}
\pagestyle{fancy}

\fancyhead{}
\fancyhead[L]{Advanced Data Mining}
\fancyhead[C]{Bj√∂rn Bebensee (2019-21343)}
\fancyhead[R]{November 20, 2019}
\fancypagestyle{plain}{%  the preset of fancyhdr 
    \fancyfoot[C]{\textbf{\thepage}} % except the center
    \renewcommand{\headrulewidth}{0pt}
    \renewcommand{\footrulewidth}{0pt}}

% spacing in itemize environments
\usepackage{enumitem}
\setitemize{noitemsep,topsep=2pt,parsep=2pt,partopsep=2pt}

% misc hyphenation
\hyphenation{page-rank}

% fancyhdr headheight
\setlength{\headheight}{15pt}

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

% ugly fake title hack
{\Large\centering
    \textbf{Beyond `Caveman Communities': Hubs and Spokes for Graph Compression and Mining}
\par}

\bigskip

\noindent
1. What is the problem that the paper wants to solve? Why is it difficult (related works)?

\begin{itemize}
    \item Problem definition: Given a real world graph, how can we compress it (and its edges)? This problem is difficult as previous work has shown that it is difficult to find `good cuts' in real world graphs due to hub nodes.
    \item Subproblem definition: Given a graph $G=(V,E)$ with adjacency matrix $A$ find permutation $\pi: V \rightarrow [n]$ such that a cost function is minimized
\end{itemize}

\noindent
2. What is the solution? What is the main idea?

\begin{itemize}
    \item Observation: Compressibility of adjacency matrix depends on node ordering
    \item Find better node ordering by exploiting the existence of high-degree hub nodes to achieve a better compression ratio
    \item The authors introduce an algorithm called \textsc{SlashBurn} to find such an ordering:
    \begin{itemize}
        \item[1.] remove top-$k$ highest centrality scoring nodes, give these hubs the lowest id,
        \item[2.] give lowest-size connected components (``spokes") the lowest IDs
        \item[3.] repeat on GCC of $G$ to get an ordering of nodes in the GCC
    \end{itemize}
\end{itemize}

\noindent
3. What is the result?

\begin{itemize}
    \item Higher rate of compression for real-world graphs that follow a power-law distribution
    \item Speed-up for matrix-vector multiplications which play an important role in many graph mining algorithms as these depend on the node ordering
\end{itemize}

\noindent
4. What is the main novelty that enabled the solution?

\begin{itemize}
    \item Observation that real-world graphs follow a power-law and that this graph structure can be used to ``shatter" the graph and iteratively order the nodes to achieve a better compression of the adjacency matrix
\end{itemize}

\noindent
5. What are the good aspects of the paper? Did you learn something from the paper?

\begin{itemize}
    \item Node reordering provides not only better compression but also speed-up for matrix-vector operations
    \item Real-world graphs can be shattered quickly
\end{itemize}

\noindent
6. What is the impact of the paper?

\begin{itemize}
    \item The authors provide a way to speed-up many graph mining algorithms that operate on large real-world graphs such as PageRank
    \item Works on graphs with ``no good cuts" where the cavemen graph compression approach will not work well
\end{itemize}

\noindent
7. Are there weaknesses/missing parts in the paper? How can you improve it?

\begin{itemize}
    \item It is claimed that \textsc{SlashBurn} is better for real-world graphs than clique-based compression approaches as there is no good cuts, but there is no direct comparison between the two
\end{itemize}

\noindent
8. How can you extend the paper?

\begin{itemize}
    \item Find a similar method which works well on graphs that do not follow a power-law as well (not all real-world graphs follow a power-law)
\end{itemize}

\noindent
9. How can you apply the technique to other data/problems?

\begin{itemize}
    \item Exploit the graph structure (or structure of data in another problem) to achieve better performance
\end{itemize}

\end{document}
