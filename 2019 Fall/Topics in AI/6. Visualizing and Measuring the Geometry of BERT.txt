The paper by Coenen et al. deals with the question which linguistic features of text are being extracted by transformers and how the information is stored internally. Previous work has found evidence of geometric representations of syntactic and linguistic features (such as parse trees). In this work the authors aim to extend this investigation into geometric representations in BERT. They find evidence that attention matrices encode grammatical representations, they show that BERT manages fine-grained distinctions of word senses and they find that this information is encoded in a low-dimensional space.

Coenen et al. investigate attention matrices to find possible representations of dependency grammar relations. They define an \emph{attention probe} as follows: given a model-wide attention vector, which is simply the concatenation of all $\alpha_{i,j}$ from every attention matrix in every attention head in every layer, an attention probe is the task of finding the encoded relation of a pair of tokens $(\text{\emph{token}}_i, \text{\emph{token}}_j)$ within the model-wide attention vector. They find that using a corpus of parsed sentences and their model-wide attention vectors (for each pair of tokens in a sentence), they can successfully train a linear classifier to recognize whether a dependency relation exists between a pair of tokens and even the type of dependency relation. This suggests that the attention factors do encode this kind of information of dependency relations.


