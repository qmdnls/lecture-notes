{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "s6_convert_face_network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKDzxwVueEkU"
      },
      "source": [
        "## Evaluate the trained network (Week 12) - Step 6\n",
        "\n",
        "####**Designed by Joon Son Chung, November 2020**\n",
        "\n",
        "In this step, we convert the PyTorch model to TensorFlow Lite format so that they can be deployed on mobile devices and on the Coral board. We use post-training quantization in this exercise.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyK0uYoip0yN"
      },
      "source": [
        "Change `pretrained_model` to the saved model that you want to convert.  We will use the validation set as the *representative dataset* for quantization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRz0Qx2JeCls"
      },
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive, files\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "import os, glob, sys, shutil, time, numpy\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# path of the data directory relative to the home folder of Google Drive\n",
        "GDRIVE_HOME = '/content/drive/My Drive'\n",
        "\n",
        "example_image     = os.path.join(GDRIVE_HOME,'MLVU/your_dataset/example.jpg')\n",
        "pretrained_model  = os.path.join(GDRIVE_HOME,'MLVU/res18_vggface1_baseline.model')\n",
        "val_zip           = os.path.join(GDRIVE_HOME,'MLVU/dataset4/val.zip') ## validation data as zip\n",
        "\n",
        "with ZipFile(val_zip, 'r') as zipObj:\n",
        "  zipObj.extractall(\"/val_set\")\n",
        "\n",
        "print('Validation files unzipped')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQqvjt_v_pk_"
      },
      "source": [
        "Install and import all necessary packages. The version of PyTorch and Tensorflow used here are `1.7.0` and `2.3.0` respectively. The code might not work with different versions of PyTorch and TF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYnM-90q_p2T"
      },
      "source": [
        "! pip install --upgrade pip\n",
        "! pip install onnx==1.8.0 \n",
        "! pip install pytorch2keras==0.2.4 \n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from pytorch2keras.converter import pytorch_to_keras\n",
        "import onnx\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbh2C1LW5vHJ"
      },
      "source": [
        "### Prepare PyTorch model and data\n",
        "First, we define the model, which must be the same as the model trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJZu029U5vZx"
      },
      "source": [
        "class FaceRecognition(torch.nn.Module):\n",
        "\tdef __init__(self, nEmbed, nClasses):\n",
        "\t    super(FaceRecognition, self).__init__()\n",
        "\t    self.__S__ \t= models.resnet18(num_classes=nEmbed)\n",
        "\t    print('Initialised Softmax Loss')\n",
        "\n",
        "\tdef forward(self, x, label=None):\n",
        "\t\tx \t= self.__S__(x)\n",
        "\n",
        "\t\treturn x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua7ODF3zCjTY"
      },
      "source": [
        "Make example input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMT6bND7Cjh7"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize(256),\n",
        "     transforms.CenterCrop([224,224]),\n",
        "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "img         = Image.open(example_image)\n",
        "img_tensor  = transform(img).unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGMuDWq4EIex"
      },
      "source": [
        "This script allows you to load parameters even if sizes of some weights have changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIYsNB1kEI1W"
      },
      "source": [
        "def loadParameters(model, path):\n",
        "\n",
        "    self_state = model.state_dict();\n",
        "    loaded_state = torch.load(path, map_location=\"cuda:0\");\n",
        "    for name, param in loaded_state.items():\n",
        "        origname = name;\n",
        "        if name not in self_state:\n",
        "\n",
        "            if name not in self_state:\n",
        "                print(\"%s is not in the model.\"%origname);\n",
        "                continue;\n",
        "\n",
        "        if self_state[name].size() != loaded_state[origname].size():\n",
        "            print(\"Wrong parameter length: %s, model: %s, loaded: %s\"%(origname, self_state[name].size(), loaded_state[origname].size()));\n",
        "            continue;\n",
        "\n",
        "        self_state[name].copy_(param);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6BxVQ_RDMoh"
      },
      "source": [
        "Load the PyTorch model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTDvUmVbDMLW"
      },
      "source": [
        "pt_model = FaceRecognition(nEmbed=512, nClasses=2700).cuda()\n",
        "pt_model.eval()\n",
        "\n",
        "loadParameters(pt_model,pretrained_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Q7MXDTqnE1"
      },
      "source": [
        "### PyTorch to Keras\n",
        "Convert from PyTorch to Keras. `change_ordering` changes the data format from `NCHW` to `NHWC`, which is necessary for conversion to TF Lite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkwJohzuqnjs"
      },
      "source": [
        "ONNX_PATH = \"./my_model.onnx\"\n",
        "TF_PATH = \"./my_tf_model.pb\" # where the representation of tensorflow model will be stored\n",
        "TFLITE_PATH = \"./my_model.tflite\"\n",
        "\n",
        "k_model = pytorch_to_keras(pt_model, img_tensor.cuda(), [(3, 224, 224,)], change_ordering=True, verbose=False, name_policy='short')  \n",
        "\n",
        "print('Converted to Keras')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT_FyoTcrALg"
      },
      "source": [
        "Ensure that the output of the Keras model is the same as the PyTorch model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ7EP1_6rAdj"
      },
      "source": [
        "## Load image using PIL and Numpy\n",
        "def numpy_loader(filename):\n",
        "  image = Image.open(filename)\n",
        "  image = image.resize((256,256),resample=Image.BILINEAR)\n",
        "  image = image.crop((16,16,240,240))\n",
        "  image = numpy.asarray(image, dtype=numpy.float32) / 255.\n",
        "  image = numpy.subtract(image, numpy.array([0.485,0.456,0.406]))\n",
        "  image = numpy.divide(image, numpy.array([0.229,0.224,0.225]))\n",
        "  image = numpy.expand_dims(image, 0) \n",
        "  image = tf.cast(tf.convert_to_tensor(image), dtype=tf.float32)\n",
        "  return image\n",
        "\n",
        "## Inference Keras model\n",
        "img_np  = numpy_loader(example_image)\n",
        "x_tf    = k_model.predict(img_np)\n",
        "\n",
        "## Inference PyTorch model\n",
        "with torch.no_grad():\n",
        "  x_pt = pt_model(img_tensor.cuda()).cpu().numpy()\n",
        "\n",
        "## Check that the outputs are the same\n",
        "from scipy import spatial\n",
        "from numpy.linalg import norm\n",
        "\n",
        "print('L2 norm (PT): %.4f'%numpy.linalg.norm(x_pt,2))\n",
        "print('L2 norm (TF): %.4f'%numpy.linalg.norm(x_tf,2))\n",
        "print('Cosine dist.: %.4f'%spatial.distance.cosine(x_pt, x_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2mXKoHzVf_"
      },
      "source": [
        "### Keras to TF Lite\n",
        "Convert the Keras model to TF Lite. This can take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Af-cvvFb6C"
      },
      "source": [
        "def tf_loader(filename):\n",
        "  image = tf.io.read_file(filename)\n",
        "  image = tf.io.decode_jpeg(image, channels=3)\n",
        "  image = tf.image.resize(image, [256, 256])\n",
        "  image = tf.image.crop_to_bounding_box(image, 16, 16, 224, 224)\n",
        "  image = tf.cast(image / 255., tf.float32)\n",
        "  image = tf.subtract(image,[0.485,0.456,0.406])\n",
        "  image = tf.divide(image,[0.229,0.224,0.225])\n",
        "  image = tf.expand_dims(image, 0) \n",
        "  return image\n",
        "  \n",
        "TFLITE_PATH = \"./my_model3.tflite\"\n",
        "\n",
        "# A generator that provides a representative dataset\n",
        "def representative_data_gen():\n",
        "  dataset_list = tf.data.Dataset.list_files('/val_set/*/*.jpg')\n",
        "  for i in range(100):\n",
        "    image = next(iter(dataset_list))\n",
        "    image = tf_loader(image)\n",
        "    yield [image]\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(k_model)\n",
        "# converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(KERAS_PATH)\n",
        "converter.experimental_new_converter = True\n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input and output tensors to uint8 (added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open(TFLITE_PATH, 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "print('Converted to TF Lite')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GC_vm56NMVD"
      },
      "source": [
        "Try performing inference to ensure that the quantization has worked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf1tWU_Mnspm"
      },
      "source": [
        "def set_input_tensor(interpreter, input):\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  tensor_index = input_details['index']\n",
        "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "  scale, zero_point = input_details['quantization']\n",
        "  input_tensor[:, :] = numpy.uint8(input / scale + zero_point)\n",
        "  \n",
        "def infer(interpreter, input):\n",
        "  set_input_tensor(interpreter, input)\n",
        "  interpreter.invoke()\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "  output = interpreter.get_tensor(output_details['index'])\n",
        "  # Outputs from the TFLite model are uint8, so we dequantize the results:\n",
        "  scale, zero_point = output_details['quantization']\n",
        "  output = numpy.array(output, dtype=numpy.float32)\n",
        "  output = scale * (output - zero_point)\n",
        "  return output\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "## test forward pass\n",
        "x_tflite   = infer(interpreter, numpy_loader(os.path.join(data_dir,'example.jpg')))\n",
        "\n",
        "## check cosine distance\n",
        "print('L2 norm (TF): %.4f'%numpy.linalg.norm(x_tflite,2))\n",
        "print('Cosine dist.: %.4f'%spatial.distance.cosine(x_tflite, x_tf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxmJq03mKQug"
      },
      "source": [
        "## Compile for Edge TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vNY1WgJKhqr"
      },
      "source": [
        "First download the [Edge TPU Compiler](https://coral.ai/docs/edgetpu/compiler/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFwq52VX6cmI"
      },
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "\n",
        "! sudo apt-get update\n",
        "\n",
        "! sudo apt-get install edgetpu-compiler\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOwXvZLbKpzk"
      },
      "source": [
        "Then compile the model for Edge TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx1Vc7-VKqBR"
      },
      "source": [
        "! edgetpu_compiler my_model3.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va7lKHKEK_Ff"
      },
      "source": [
        "Download the converted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuN8cQEvK-cR"
      },
      "source": [
        "files.download('my_model3_edgetpu.tflite')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}